{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "* [x] Setup WandB\n",
    "* [x] Split data\n",
    "* Network:\n",
    "  - [ ] Original\n",
    "  * Mine\n",
    "    1. [x] ReLU\n",
    "    2. [x] Compression / Expansion\n",
    "    3. [x] Batch Norm\n",
    "    4. [x] Squeeze Excitation\n",
    "    5. [x] Transposed vs Upsampled\n",
    "    5. [x] EfficientNet Encoder\n",
    "* Augmentation:\n",
    "  - Preprocessing:\n",
    "    * [x] Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:53:44.562581Z",
     "iopub.status.busy": "2023-09-10T00:53:44.562203Z",
     "iopub.status.idle": "2023-09-10T00:53:45.657400Z",
     "shell.execute_reply": "2023-09-10T00:53:45.656077Z",
     "shell.execute_reply.started": "2023-09-10T00:53:44.562552Z"
    }
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:53:45.660499Z",
     "iopub.status.busy": "2023-09-10T00:53:45.660095Z",
     "iopub.status.idle": "2023-09-10T00:53:46.760368Z",
     "shell.execute_reply": "2023-09-10T00:53:46.759021Z",
     "shell.execute_reply.started": "2023-09-10T00:53:45.660465Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/MicheleDamian/UNetBox.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:53:46.763027Z",
     "iopub.status.busy": "2023-09-10T00:53:46.762485Z",
     "iopub.status.idle": "2023-09-10T00:53:46.769557Z",
     "shell.execute_reply": "2023-09-10T00:53:46.768306Z",
     "shell.execute_reply.started": "2023-09-10T00:53:46.762980Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, f'/kaggle/working/UNetBox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-10T00:53:46.783759Z",
     "iopub.status.busy": "2023-09-10T00:53:46.783352Z",
     "iopub.status.idle": "2023-09-10T00:54:02.356581Z",
     "shell.execute_reply": "2023-09-10T00:54:02.355346Z",
     "shell.execute_reply.started": "2023-09-10T00:53:46.783728Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import cv2\n",
    "import multiprocessing\n",
    "import wandb\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import product, chain\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from path import Path\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torchvision.ops.misc import Conv2dNormActivation, SqueezeExcitation\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from unetbox.net import UNetBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:17.634448Z",
     "iopub.status.busy": "2023-09-10T00:55:17.633871Z",
     "iopub.status.idle": "2023-09-10T00:55:23.139398Z",
     "shell.execute_reply": "2023-09-10T00:55:23.137958Z",
     "shell.execute_reply.started": "2023-09-10T00:55:17.634400Z"
    }
   },
   "outputs": [],
   "source": [
    "!wandb login {secret_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.142778Z",
     "iopub.status.busy": "2023-09-10T00:55:23.142387Z",
     "iopub.status.idle": "2023-09-10T00:55:23.157602Z",
     "shell.execute_reply": "2023-09-10T00:55:23.156390Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.142744Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    project: str                 = 'Unet Ablation'\n",
    "    session_id: str              = None\n",
    "    seed: int                    = 2023\n",
    "    n_folds: int                 = 3\n",
    "    learning_rate: float         = 2e-3       # This can be changed after running the Tuner\n",
    "    kaggle_path: Path            = Path('./google-research-identify-contrails-reduce-global-warming')\n",
    "    input_path: Path             = Path('./dataset')\n",
    "    output_path: Path            = Path('.')\n",
    "    n_channels: int              = 3          # Number of channels in the images in the dataset\n",
    "    timeindex: tuple[int]        = (4, )\n",
    "    input_size: tuple[int]       = (256, 256) # The size of the first layer's input\n",
    "    data_mean: tuple[float]      = (275.65, 0.98859, -2.8341)\n",
    "    data_std: tuple[float]       = (14.714, 1.549, 0.93514)\n",
    "    batch_size: int              = 64\n",
    "    accumulate_grad: int         = 1\n",
    "    num_epochs: int              = 15\n",
    "    model_params: dict[str, int] = field(default_factory=dict)\n",
    "    align_motion: str            = None\n",
    "        \n",
    "Config.model_params = {\n",
    "    'depth': 4, \n",
    "    'expansion': 16, \n",
    "    'base_chn': Config.n_channels * len(Config.timeindex), \n",
    "    'activation': nn.SiLU, \n",
    "    'encoder': 'default',\n",
    "    'expansion_layer': True,\n",
    "    'norm_layer': True,\n",
    "    'convup_layer': True,\n",
    "    'se_block': True\n",
    "} \n",
    "\n",
    "Config.session_id = Config.session_id or datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.690811Z",
     "iopub.status.busy": "2023-09-10T00:55:23.690423Z",
     "iopub.status.idle": "2023-09-10T00:55:23.706836Z",
     "shell.execute_reply": "2023-09-10T00:55:23.705420Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.690779Z"
    }
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(Config.seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.708436Z",
     "iopub.status.busy": "2023-09-10T00:55:23.708107Z",
     "iopub.status.idle": "2023-09-10T00:55:23.720667Z",
     "shell.execute_reply": "2023-09-10T00:55:23.719375Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.708398Z"
    }
   },
   "outputs": [],
   "source": [
    "class RLE():\n",
    "    @staticmethod\n",
    "    def encode(mask):\n",
    "        \n",
    "        m = np.zeros(mask.size + 2, dtype=mask.dtype)\n",
    "        m[1:-1] = mask.T.flatten()\n",
    "        \n",
    "        start = np.where(m[:-1] != m[1:])[0] + 1\n",
    "        length = start[1:] - start[:-1]\n",
    "        return list(zip(start, length))[::2]\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(rle, height, width):\n",
    "        \n",
    "        mask = np.zeros(height * width, dtype=np.uint8)\n",
    "\n",
    "        if type(rle) != list: rle = []\n",
    "\n",
    "        for s, l in rle: mask[s-1:s+l-1] = 1\n",
    "\n",
    "        return mask.reshape(height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.723326Z",
     "iopub.status.busy": "2023-09-10T00:55:23.722973Z",
     "iopub.status.idle": "2023-09-10T00:55:23.736392Z",
     "shell.execute_reply": "2023-09-10T00:55:23.735228Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.723298Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_contrail_size(paths):\n",
    "    \n",
    "    contrail_size = []\n",
    "\n",
    "    for mask_path in tqdm(paths):\n",
    "\n",
    "        mask = np.load(mask_path / 'human_pixel_masks.npy')\n",
    "        contrail_size.append(mask.sum())\n",
    "        \n",
    "    return contrail_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.738452Z",
     "iopub.status.busy": "2023-09-10T00:55:23.737930Z",
     "iopub.status.idle": "2023-09-10T00:55:23.750069Z",
     "shell.execute_reply": "2023-09-10T00:55:23.748926Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.738405Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_ash_image(basepath, timeindex=4, clip=False):\n",
    "    \n",
    "    band_12_path = basepath / 'band_15.npy'\n",
    "    band_11_path = basepath / 'band_14.npy'\n",
    "    band_8_path = basepath / 'band_11.npy'\n",
    "    \n",
    "    band_12 = np.load(band_12_path)[..., timeindex].astype(np.float32)\n",
    "    band_11 = np.load(band_11_path)[..., timeindex].astype(np.float32)\n",
    "    band_8 = np.load(band_8_path)[..., timeindex].astype(np.float32)\n",
    "    \n",
    "    chn_0 = band_11\n",
    "    chn_1 = band_11 - band_8\n",
    "    chn_2 = band_12 - band_11\n",
    "        \n",
    "    if clip:\n",
    "        chn_0 = chn_0.clip(min=243, max=303)\n",
    "        chn_1 = chn_1.clip(min=-4, max=5)\n",
    "        chn_2 = chn_2.clip(min=-4, max=2)\n",
    "        \n",
    "    return np.stack((chn_0, chn_1, chn_2), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.752196Z",
     "iopub.status.busy": "2023-09-10T00:55:23.751713Z",
     "iopub.status.idle": "2023-09-10T00:55:23.770249Z",
     "shell.execute_reply": "2023-09-10T00:55:23.769272Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.752157Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContrailsDataset(Dataset):\n",
    "    def __init__(self, \n",
    "        paths, \n",
    "        transforms, \n",
    "        testset=False, \n",
    "        timeindex=[4], \n",
    "        align=None,\n",
    "        storage=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.paths = paths\n",
    "        self.transforms = transforms\n",
    "        self.testset = testset\n",
    "        self.timeindex = timeindex\n",
    "        self.align = align\n",
    "        self.storage = storage\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        dst_dir = self.storage or Path('/')\n",
    "        flow_path = dst_dir / 'train' / self.paths[idx].name / 'flow.npy'\n",
    "        \n",
    "        flow = np.load(flow_path) if flow_path.exists() else None\n",
    "            \n",
    "        input = create_ash_image(self.paths[idx], timeindex=self.timeindex)\n",
    "\n",
    "        if self.align: \n",
    "            for t in range(len(self.timeindex) - 1):\n",
    "                input[..., t, :], out_flow = align_channels(\n",
    "                    input[..., t, :], \n",
    "                    input[..., -1, :], \n",
    "                    flow=flow,\n",
    "                    motion=self.align, \n",
    "                    iterations=10\n",
    "                )\n",
    "                         \n",
    "        if self.storage and flow is None: \n",
    "            if not flow_path.parent.exists(): flow_path.parent.makedirs()\n",
    "            np.save(flow_path, out_flow)\n",
    "        \n",
    "        if len(input.shape) > 3: input = input.reshape((*input.shape[:2], -1))\n",
    "        \n",
    "        if not self.testset:\n",
    "            mask_path = self.paths[idx] / 'human_pixel_masks.npy'\n",
    "            mask = np.load(mask_path).squeeze().astype(np.float32)\n",
    "        else:\n",
    "            mask = np.zeros(input.shape[:2], dtype=np.float32)\n",
    "        \n",
    "        if self.transforms:\n",
    "            trs = self.transforms(image=input, mask=mask)\n",
    "            input, mask = trs['image'], trs['mask']\n",
    "                \n",
    "        return input, mask[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.772124Z",
     "iopub.status.busy": "2023-09-10T00:55:23.771461Z",
     "iopub.status.idle": "2023-09-10T00:55:23.789059Z",
     "shell.execute_reply": "2023-09-10T00:55:23.787798Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.772092Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, \n",
    "        train_df, \n",
    "        valid_df,\n",
    "        input_size, \n",
    "        data_mean, \n",
    "        data_std, \n",
    "        num_cpus=os.cpu_count(),\n",
    "        batch_size=32,\n",
    "        transforms=None,\n",
    "        timeindex=(4,),\n",
    "        align=None,\n",
    "        storage=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_df, self.valid_df = train_df, valid_df\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.num_cpus = num_cpus\n",
    "        self.timeindex = timeindex\n",
    "        self.align = align\n",
    "        self.storage = storage\n",
    "        \n",
    "        self.data_mean, self.data_std = data_mean * len(timeindex), data_std * len(timeindex)\n",
    "        \n",
    "        self.transforms_train = transforms or A.Compose([\n",
    "            A.Normalize(self.data_mean, self.data_std, max_pixel_value=1.0),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.transforms_valid = A.Compose([\n",
    "            A.Normalize(self.data_mean, self.data_std, max_pixel_value=1.0),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        trainset = ContrailsDataset(\n",
    "            self.train_df['path'].values, \n",
    "            self.transforms_train, \n",
    "            timeindex=self.timeindex,\n",
    "            align=self.align,\n",
    "            storage=self.storage\n",
    "        )\n",
    "        num_batches = len(trainset) // self.batch_size\n",
    "        k_fold = StratifiedKFold(n_splits=num_batches, shuffle=True)\n",
    "        batch_sampler = list(fold for _, fold in k_fold.split(self.train_df, self.train_df['bin']))\n",
    "        return DataLoader(trainset, batch_sampler=batch_sampler, num_workers=self.num_cpus)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valset = ContrailsDataset(\n",
    "            self.valid_df['path'].values, \n",
    "            self.transforms_valid, \n",
    "            timeindex=self.timeindex,\n",
    "            align=self.align,\n",
    "            storage=self.storage\n",
    "        )\n",
    "        return DataLoader(valset, batch_size=self.batch_size, num_workers=self.num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T01:00:19.804670Z",
     "iopub.status.busy": "2023-09-10T01:00:19.804219Z",
     "iopub.status.idle": "2023-09-10T01:00:19.820420Z",
     "shell.execute_reply": "2023-09-10T01:00:19.819461Z",
     "shell.execute_reply.started": "2023-09-10T01:00:19.804637Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model, total_steps, learning_rate, criterion):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = UNetBox(**model) if isinstance(model, dict) else model\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate_max = learning_rate\n",
    "        self.total_steps = total_steps\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "         \n",
    "        self.valid_loss = 0.\n",
    "        self.valid_num_batches = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()), \n",
    "            lr=self.learning_rate_max / 5e1\n",
    "        )\n",
    "        \n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.learning_rate_max,\n",
    "            div_factor=10.,\n",
    "            final_div_factor=1.,\n",
    "            total_steps=self.total_steps\n",
    "        )\n",
    "\n",
    "        config = {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'strict': False\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return config\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # Add training metrics\n",
    "        self.log(f'train/{self.criterion.func.__name__}', loss, logger=True)\n",
    "\n",
    "        return loss\n",
    "            \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if not self.logger or not isinstance(self.logger, WandbLogger): return\n",
    "        \n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(inputs)\n",
    "        \n",
    "        self.valid_loss += self.criterion(outputs, labels).cpu().item()\n",
    "        self.valid_num_batches += 1\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # Add validation metrics\n",
    "        loss = self.valid_loss / max(1, self.valid_num_batches)\n",
    "        self.log(f'validation/{self.criterion.func.__name__}', loss, logger=True)\n",
    "        \n",
    "        self.valid_loss = 0.\n",
    "        self.valid_num_batches = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.847084Z",
     "iopub.status.busy": "2023-09-10T00:55:23.846310Z",
     "iopub.status.idle": "2023-09-10T00:55:23.864051Z",
     "shell.execute_reply": "2023-09-10T00:55:23.862281Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.847048Z"
    }
   },
   "outputs": [],
   "source": [
    "def continuous_dice_loss(inputs, targets, reduction='none', continuous=True, dim=-1):\n",
    "\n",
    "    inputs = inputs.sigmoid()\n",
    "    \n",
    "    if not continuous: inputs = inputs > .5\n",
    "    \n",
    "    # Flatten label and prediction tensors\n",
    "    start_dim = 1 if reduction == 'none' else 0\n",
    "    inputs = inputs.flatten(start_dim=start_dim)\n",
    "    targets = targets.flatten(start_dim=start_dim)\n",
    "    \n",
    "    intersection = (inputs * targets).sum(dim=dim)\n",
    "    union = inputs.sum(dim=dim) + targets.sum(dim=dim)\n",
    "    \n",
    "    cDC = 1. - 2. * intersection / union\n",
    "    \n",
    "    return cDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.866387Z",
     "iopub.status.busy": "2023-09-10T00:55:23.865730Z",
     "iopub.status.idle": "2023-09-10T00:55:23.878157Z",
     "shell.execute_reply": "2023-09-10T00:55:23.876838Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.866343Z"
    }
   },
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Normalize(\n",
    "        Config.data_mean * len(Config.timeindex), \n",
    "        Config.data_std * len(Config.timeindex), \n",
    "        max_pixel_value=1.0),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:23.882402Z",
     "iopub.status.busy": "2023-09-10T00:55:23.880714Z",
     "iopub.status.idle": "2023-09-10T00:55:24.737046Z",
     "shell.execute_reply": "2023-09-10T00:55:24.735938Z",
     "shell.execute_reply.started": "2023-09-10T00:55:23.882290Z"
    }
   },
   "outputs": [],
   "source": [
    "data_paths = (Config.kaggle_path / 'train').listdir()\n",
    "data_paths += (Config.kaggle_path / 'validation').listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:24.739288Z",
     "iopub.status.busy": "2023-09-10T00:55:24.738799Z",
     "iopub.status.idle": "2023-09-10T00:55:24.747253Z",
     "shell.execute_reply": "2023-09-10T00:55:24.745848Z",
     "shell.execute_reply.started": "2023-09-10T00:55:24.739245Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = {\n",
    "    0: (0, 0), \n",
    "    1: (1, 99), \n",
    "    2: (100, 328), \n",
    "    3: (329, 907), \n",
    "    4: (908, 2**16)\n",
    "}\n",
    "\n",
    "binning = lambda x: max(k if v[0] <= x <= v[1] else 0 for k, v in bins.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:55:24.749587Z",
     "iopub.status.busy": "2023-09-10T00:55:24.749123Z",
     "iopub.status.idle": "2023-09-10T00:59:22.339825Z",
     "shell.execute_reply": "2023-09-10T00:59:22.338641Z",
     "shell.execute_reply.started": "2023-09-10T00:55:24.749541Z"
    }
   },
   "outputs": [],
   "source": [
    "contrail_size = get_contrail_size(data_paths)\n",
    "\n",
    "data_df = pd.DataFrame(data=zip(data_paths, contrail_size), columns=['path', 'contrail_size'])\n",
    "data_df['bin'] = data_df['contrail_size'].map(binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:59:22.341463Z",
     "iopub.status.busy": "2023-09-10T00:59:22.341137Z",
     "iopub.status.idle": "2023-09-10T00:59:22.346742Z",
     "shell.execute_reply": "2023-09-10T00:59:22.345676Z",
     "shell.execute_reply.started": "2023-09-10T00:59:22.341434Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = partial(sigmoid_focal_loss, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:59:33.412945Z",
     "iopub.status.busy": "2023-09-10T00:59:33.412592Z",
     "iopub.status.idle": "2023-09-10T00:59:33.422353Z",
     "shell.execute_reply": "2023-09-10T00:59:33.421113Z",
     "shell.execute_reply.started": "2023-09-10T00:59:33.412888Z"
    }
   },
   "outputs": [],
   "source": [
    "if not Config.learning_rate:\n",
    "    \n",
    "    data = DataModule(\n",
    "        data_df, \n",
    "        data_df,\n",
    "        Config.input_size, \n",
    "        Config.data_mean, \n",
    "        Config.data_std, \n",
    "        batch_size=Config.batch_size,\n",
    "        transforms=transforms,\n",
    "        timeindex=Config.timeindex,\n",
    "        align=Config.align_motion\n",
    "    )\n",
    "    \n",
    "    total_steps = math.ceil(len(data.train_dataloader()) / Config.accumulate_grad) * Config.num_epochs\n",
    "    model = Model(Config.model_params, total_steps, 1., loss_func)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        precision='16-mixed',\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        max_epochs=Config.num_epochs,\n",
    "        accumulate_grad_batches=Config.accumulate_grad\n",
    "    )\n",
    "    tuner = pl.tuner.tuning.Tuner(trainer)\n",
    "    lr_finder = tuner.lr_find(model=model, datamodule=data, min_lr=1e-5)\n",
    "\n",
    "    fig = lr_finder.plot(suggest=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T00:59:33.424337Z",
     "iopub.status.busy": "2023-09-10T00:59:33.423940Z",
     "iopub.status.idle": "2023-09-10T00:59:33.439143Z",
     "shell.execute_reply": "2023-09-10T00:59:33.437893Z",
     "shell.execute_reply.started": "2023-09-10T00:59:33.424306Z"
    }
   },
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=Config.n_folds, shuffle=True)\n",
    "folds = enumerate(k_fold.split(data_df, data_df['bin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T01:01:15.709223Z",
     "iopub.status.busy": "2023-09-10T01:01:15.708770Z",
     "iopub.status.idle": "2023-09-10T01:28:00.700720Z",
     "shell.execute_reply": "2023-09-10T01:28:00.698371Z",
     "shell.execute_reply.started": "2023-09-10T01:01:15.709185Z"
    }
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(\n",
    "    iterable=folds,\n",
    "    desc='Fold',\n",
    "    total=Config.n_folds,\n",
    "    position=0\n",
    ")\n",
    "\n",
    "for fold, (train_index, valid_index) in pbar:\n",
    "        \n",
    "    train_df, valid_df = data_df.iloc[train_index], data_df.iloc[valid_index]\n",
    "    \n",
    "    data = DataModule(\n",
    "        train_df, \n",
    "        valid_df,\n",
    "        Config.input_size, \n",
    "        Config.data_mean, \n",
    "        Config.data_std, \n",
    "        batch_size=Config.batch_size,\n",
    "        transforms=transforms,\n",
    "        timeindex=Config.timeindex,\n",
    "        align=Config.align_motion\n",
    "    )\n",
    "    \n",
    "    total_steps = math.ceil(len(data.train_dataloader()) / Config.accumulate_grad) * Config.num_epochs\n",
    "    model = Model(Config.model_params, total_steps, Config.learning_rate, loss_func)\n",
    "\n",
    "    # This is to close the previous run and start a new one; \n",
    "    # wandb_logger.finalize('success') doesn't work as expected\n",
    "    wandb.finish()\n",
    "    \n",
    "    wandb_logger = WandbLogger(\n",
    "        project=Config.project,\n",
    "        group=Config.session_id,\n",
    "        name=f'{Config.session_id}_{fold}',\n",
    "        log_model='all'\n",
    "    )\n",
    "    \n",
    "    wandb_logger.watch(model, log='all')\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        precision='16-mixed',\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        max_epochs=Config.num_epochs,\n",
    "        accumulate_grad_batches=Config.accumulate_grad,\n",
    "        deterministic=True,\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(logging_interval='step'), \n",
    "            ModelCheckpoint(monitor=f'validation/{model.criterion.func.__name__}', mode='min', save_top_k=3)\n",
    "        ],\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=model, datamodule=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
